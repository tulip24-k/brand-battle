{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1pqaJkgFFrElV1_y9pt8AuuyaPA135KxY","timestamp":1766500042230},{"file_id":"1W08tkMVhMpLmzFln7DuG1-F-nmdqvcvc","timestamp":1766174428279}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Case Study: Customer Churn Prediction**\n","\n","You are working as a data analyst at a subscription-based company.  \n","The company has noticed that many customers cancel their subscriptions and wants to understand **who is likely to churn and why**.\n","\n","You are given historical customer data containing subscription details, usage behavior, and a label indicating whether the customer churned or not.\n","\n","Your task is to use **logistic regression** to predict whether a customer will churn.\n","\n","Run the code cell below to access the dataset from the Finance & Accounting Team."],"metadata":{"id":"AiDFIEtnL-Fa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQ1s1PojEEvm"},"outputs":[],"source":["import kagglehub\n","import os\n","\n","path = kagglehub.dataset_download(\"safrin03/predictive-analytics-for-customer-churn-dataset\")\n","os.listdir(path)"]},{"cell_type":"markdown","source":["Let's have a look at the dataset"],"metadata":{"id":"7kc4hHCrNhvW"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(os.path.join(path, \"train.csv\"))\n","\n","# Print the first 5 rows of the dataset\n","print(df.head())"],"metadata":{"id":"Ra2ca77EETJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's have a look at the features we have."],"metadata":{"id":"k2kel4snL0yT"}},{"cell_type":"code","source":["# Print the names of all the columns in the dataset\n","print(df.columns)"],"metadata":{"id":"4_9RaPwQLXtu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data can also have problems!\n","\n","After reviewing the data, the **Finance & Accounting team** reports that some customer attributes were incorrectly captured due to issues in the billing and content-logging systems.\n","\n","As a result, you are instructed to **remove the following columns from the dataset and not use them for your analysis**:\n","- `PaymentMethod`\n","- `PaperlessBilling`\n","- `DeviceRegistered`\n","- `GenrePreference`\n","\n","Make sure these columns are **dropped in-place** (set inplace = True when you use drop method)"],"metadata":{"id":"3BWez-rBN8IV"}},{"cell_type":"code","source":["# Drop the mentioned columns\n","df.drop(\n","    [\"PaymentMethod\", \"PaperlessBilling\", \"DeviceRegistered\", \"GenrePreference\"],\n","    axis=1,\n","    inplace=True\n",")\n","\n"],"metadata":{"id":"XKj6zyuSJqtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before training the model, review all remaining features and ask yourself:\n","\n","- Does this variable contain information about customer behavior, or is it only an identifier?\n","- Would knowing this value help predict churn for a **new** customer?\n","\n","Identify any such column(s) and drop them inplace as well."],"metadata":{"id":"XxxGBJ4yQXqV"}},{"cell_type":"code","source":["# Drop such columns\n","df.drop([\"CustomerID\"],\n","        axis=1, inplace=True)\n","print(df.columns)"],"metadata":{"id":"6rnUEVxaFDgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if there are any missing values in the dataset."],"metadata":{"id":"gW13kwKMhahH"}},{"cell_type":"code","source":["# Check for NA values\n","print(df.isna().any())"],"metadata":{"id":"-bc7XpKeheFT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Encoding Categorical Variables for Modeling**\n","\n","During data preparation, you notice that several features in the dataset are **categorical** (they contain labels instead of numbers).  \n","Since logistic regression works only with numerical inputs, these variables must be **encoded** before modeling.\n","\n","\n","\n","### Why Encoding Is Important\n","- Machine learning models cannot interpret text values such as *“Basic”* or *“Yes”*\n","- Encoding converts categories into numbers **without introducing false meaning**\n","- Poor encoding can mislead the model and hurt predictions\n","\n","\n","\n","### Encoding Rules to Follow\n","\n","1. **Columns with exactly two categories**  \n","   Encode them using **0 / 1**  \n","   - Example: `No → 0`, `Yes → 1`\n","\n","2. **Columns with more than two categories**  \n","   Use **One-Hot Encoding**, then **drop the original column**\n","\n","\n","\n","### What Is One-Hot Encoding?\n","\n","One-hot encoding represents each category as its **own binary column**.\n","\n","Instead of giving categories numbers (which can imply order), it creates **one column per category**.\n","\n","Example:\n","\n","SubscriptionType can be Basic, Standard or Premium\n","\n","Create 3 separate columns: SubscriptionTypeBasic, SubscriptionTypeStandard, SubscriptionTypePremium.\n","\n","If SubscriptionType is Basic for some row, then for that row, set SubscriptionTypeBasic = 1, SubscriptionTypeStandard = 0, SubscriptionTypePremium = 0. Do similarly for all the rows then drop the SubscriptionType column inplace.\n","\n"],"metadata":{"id":"d1vQ9P_zi43i"}},{"cell_type":"markdown","source":["# **Hint from the Analytics Team**\n","\n","One of the categorical columns appears to have three values.  \n","Before applying one-hot encoding, think carefully about what these values mean.\n","\n","Ask yourself:\n","- Are these three values mutually exclusive choices?\n","- Or do they represent **different combinations of the same underlying options**?\n","\n","Do you really need three different columns for that feature or fewer can be sufficient?"],"metadata":{"id":"4_qJEFOsvZNs"}},{"cell_type":"code","source":["# Encode the 6 categorical features\n","# For binary features, replace the text values with 0 / 1\n","# For mroe than 2 categories, create separate binary columns for each category and drop the original column\n","binary_cols = [\"MultiDeviceAccess\",\"ParentalControl\",\"SubtitlesEnabled\"]\n","for col in binary_cols:\n","    df[col] = df[col].map({\"No\": 0, \"Yes\": 1})\n","\n","df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})\n","\n","print(df.isna().sum()) #Verify no NaNs exist\n","\n","subscription_dummies = pd.get_dummies(\n","    df[\"SubscriptionType\"],\n","    prefix=\"SubscriptionType\"\n",")\n","\n","df = pd.concat([df, subscription_dummies], axis=1)\n","df.drop(\"SubscriptionType\", axis=1, inplace=True)\n","\n","content_dummies = pd.get_dummies(\n","    df[\"ContentType\"],\n","    prefix=\"ContentType\"\n",")\n","\n","df = pd.concat([df, content_dummies], axis=1)\n","df.drop(\"ContentType\", axis=1, inplace=True)\n"],"metadata":{"id":"TA4PSOsAhU20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **What is Scikit-learn?**\n","\n","**Scikit-learn** is a Python library that helps you build machine learning models easily.  \n","Instead of coding all the mathematical formulas yourself, scikit-learn provides reliable, ready-made implementations.\n","\n","It offers tools to:\n","- Prepare data (splitting, scaling, encoding, etc.)\n","- Train machine learning models\n","- Evaluate how well models perform\n","- Avoid common mistakes in machine learning workflows\n","\n","Scikit-learn is one of the most widely used machine learning libraries.  \n","Going forward, we will use scikit-learn step by step to complete our task.\n","\n"],"metadata":{"id":"D0vri1a82Yz2"}},{"cell_type":"markdown","source":["# **Preparing Data for Modeling**\n","\n","1. Create\n","   - **x** → all feature columns used for prediction (a new dataframe same as current df just without the churn column)\n","   - **y** → the target column\n","\n","\n","2. Once x and y are defined, split them into training and test sets using scikit-learn's train_test_split method\n","- **Training set** - used to train the model\n","- **Test set** - used to evaluate the model on unseen data\n","\n","Follow these rules:\n","- Keep **20%** of the data for testing\n","- The result should give you **four outputs**:\n","  - training features\n","  - test features\n","  - training labels\n","  - test labels\n","\n","Since this is your first time using scikit-learn, you're given the splitting code below, but understand what are the inputs and outputs of the train_test_split method.\n"],"metadata":{"id":"HWiIsC4C3npX"}},{"cell_type":"code","source":["x =df.drop(\"Churn\", axis=1)  # entire df excluding churn column\n","y =df[\"Churn\"] # the churn column\n","\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"],"metadata":{"id":"fCobPU3c2Yfh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **A Note from the Statistics Team**\n","\n","Before training the churn prediction model, the statisticians review your data and point out an important issue.\n","\n","They observe that some numerical features have **much larger values** than others. These large-scale features could influence the model more than other features.\n","\n","To avoid this, you are asked to **normalize all continuous numerical features** so that they are on a similar scale.  \n","Use **standardization**, which adjusts each feature to have:\n","- a mean of 0  \n","- a standard deviation of 1  \n","\n","The statisticians also clarify that **binary features (encoded as 0 and 1)** should **not** be normalized. These already represent simple yes/no information and work correctly without scaling.\n","\n","You'll use scikit-learn's StandardScaler for this task."],"metadata":{"id":"CRINFZbPzagc"}},{"cell_type":"code","source":["continuous_cols = [\"AccountAge\", \"MonthlyCharges\", ] # Fill the rest as well\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","\n","x_train[continuous_cols] = scaler.fit_transform(x_train[continuous_cols])\n","# Do the same for x_test\n","x_test[continuous_cols] = scaler.transform(x_test[continuous_cols])"],"metadata":{"id":"EinP7L3Fu_cZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Imbalanced data**\n","\n","First, check the number of samples in each output class (churn vs non-churn).  \n","Determine whether there is a **severe class imbalance**.\n","\n","If a strong imbalance exists, apply **SMOTE** to balance the classes, as discussed in the previous session.\n","\n","You're given the code for SMOTE since it's your first time, but carefully go through it and understand the inputs and outputs."],"metadata":{"id":"TgjZ2QBW_CWv"}},{"cell_type":"code","source":["# Print the counts of each output class in the training data\n","print(y_train.value_counts())"],"metadata":{"id":"gzXN1WluyOfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","smote = SMOTE(random_state=42)\n","\n","x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n","\n","# Print the counts of each output class again after SMOTE\n","print(y_train_resampled.value_counts())"],"metadata":{"id":"-IeraYHk-7Y9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Time to train the model, finally!"],"metadata":{"id":"WeR-5apQAZJa"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","model = LogisticRegression(max_iter=1000)\n","\n","# Use mode.fit() method, you have to look up what are the arguments that go into this method\n","# Make sure you pass resampled data i.e. the data after SMOTE\n","model.fit(x_train_resampled, y_train_resampled)"],"metadata":{"id":"_6xpyXD2AUIa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test your model on the testing data"],"metadata":{"id":"ZDvuFzWACm8v"}},{"cell_type":"code","source":["y_pred = model.predict(x_test) # use model.predict() method, look up what goes as argument in that method\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n","\n","accuracy = accuracy_score(y_test, y_pred)      # use accuracy_score() method, look up the arguments\n","conf_matrix = confusion_matrix(y_test, y_pred) # use confusion_matrix() method, look up the arguments\n","precision = precision_score(y_test, y_pred)    # use precision_score() method, look up the arguments\n","recall = recall_score(y_test, y_pred)          # use recall_score() method, look up the arguments\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)"],"metadata":{"id":"deT3VCRICOKi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A bit of Consulting.... Since it's an ICG Project\n","\n","Now, you can take some time to think about which metrics matter more to your business specifically. After this churn analysis, the business will ultimately try to retain the churners by contacting them or providing some offers to them. False Negatives are churners you missed, so you lose customers while False Positives are non-churners who you classified as churners, so your business spends additional amount contacting them.\n","\n","It's upto your business - whether revenue loss due to losing customers is more significant or marketing cost to the churners is more significant. That's the precision-recall tradeoff we discussed."],"metadata":{"id":"NtST2TYiKzOz"}}]}